Environment:
	Python: 3.6.8
	PyTorch: 1.8.1+cu111
	Torchvision: 0.9.1+cu111
	CUDA: 11.1
	CUDNN: 8005
	NumPy: 1.19.5
	PIL: 8.3.1
Args:
	algorithm: KL
	checkpoint_freq: None
	data_dir: /scratch/local/ssd/tuan/data/
	dataset: RotatedMNIST
	epochs: None
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 2
	output_dir: ./results_sweep/98600a7a190f8047fda232d7aa9e5f96
	save_model_every_checkpoint: False
	seed: 1088506346
	skip_model_save: False
	steps: None
	task: domain_adaptation
	test_envs: [1]
	train_envs: [0]
	trial_seed: 1
	uda_holdout_fraction: 0
HParams:
	augment_softmax: 0.01
	batch_size: 256
	class_balanced: False
	data_augmentation: True
	kl_reg: 0.3
	kl_reg_aux: 0.1
	lr: 0.0025521952552627065
	nonlinear_classifier: True
	num_samples: 20
	resnet18: True
	resnet_dropout: 0.5
	specify_zdim: False
	weight_decay: 0.0
	z_dim: 128
cuda
/homes/55/tuan/installations/tnew/lib64/python3.6/site-packages/torchvision/transforms/functional.py:936: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead
  "Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead"
env0_out_acc  env1_in_acc   env1_out_acc  epoch         kl            kl_aux        loss          step          step_time    
0.1165880840  0.0958860081  0.1024432062  0.0000000000  0.0478002429  0.0945092738  2.3126611710  0             0.5289182663 
0.0810115731  0.0906363831  0.1003000429  1.0000000000  0.0028812968  0.0028937202  4.1845899622  36            0.0593016479 
0.0810115731  0.0906363831  0.1003000429  2.0000000000  0.0002549448  0.0001622720  4.1622097360  72            0.0582771500 
0.0810115731  0.0906363831  0.1003000429  3.0000000000  0.0005014787  0.0001684477  4.1817881266  108           0.0566086637 
0.0810115731  0.0906363831  0.1003000429  4.0000000000  0.0003303140  0.0002245166  4.1749357118  144           0.0560726523 
0.0810115731  0.0906363831  0.1003000429  5.0000000000  0.0000385311  0.0001352554  4.1715095043  180           0.0574617982 
0.0810115731  0.0906363831  0.1003000429  6.0000000000  -0.000015437  0.0000897886  4.1910879612  216           0.0567496551 
0.0810115731  0.0906363831  0.1003000429  7.0000000000  -0.000122949  0.0001001010  4.1675937573  252           0.0553975900 
0.0810115731  0.0906363831  0.1003000429  8.0000000000  0.0000845906  0.0001067552  4.1866827276  288           0.0574691693 
0.0810115731  0.0906363831  0.1003000429  9.0000000000  -0.000053014  -0.000235326  4.1954929431  324           0.0560916795 
0.0810115731  0.0906363831  0.1003000429  10.000000000  0.0001215612  0.0002043140  4.1636780832  360           0.0553433895 
0.0810115731  0.0906363831  0.1003000429  11.000000000  0.0001836460  0.0001774935  4.1744461722  396           0.0548262331 
0.0810115731  0.0906363831  0.1003000429  12.000000000  0.0000170171  -0.000017664  4.1817881134  432           0.0559026003 
0.0810115731  0.0906363831  0.1003000429  13.000000000  0.0000546674  0.0000037212  4.2003876368  468           0.0564395388 
0.0810115731  0.0906363831  0.1003000429  14.000000000  0.0000785821  -0.000061323  4.1764040656  504           0.0557966365 
0.0810115731  0.0906363831  0.1003000429  15.000000000  0.0000675701  -0.000083273  4.1754251321  540           0.0565693511 
0.0810115731  0.0906363831  0.1003000429  16.000000000  0.0000475471  0.0000734594  4.1783619722  576           0.0563989282 
0.0810115731  0.0906363831  0.1003000429  17.000000000  -0.000045342  0.0000522882  4.1945141289  612           0.0593229201 
0.0810115731  0.0906363831  0.1003000429  18.000000000  0.0000568289  0.0000883912  4.1661254300  648           0.0577237606 
0.0810115731  0.0906363831  0.1003000429  19.000000000  0.0000154293  -0.000094436  4.1739568710  684           0.0561535623 
0.0810115731  0.0906363831  0.1003000429  20.000000000  0.0000463012  0.0000614557  4.1915773153  720           0.0562815136 
0.0810115731  0.0906363831  0.1003000429  21.000000000  0.0000610501  -0.000010390  4.2145818207  756           0.0566245847 
0.0810115731  0.0906363831  0.1003000429  22.000000000  -0.000000730  -0.000000577  4.1768935124  792           0.0564962626 
0.0810115731  0.0906363831  0.1003000429  23.000000000  0.0000148507  0.0000725297  4.1656360229  828           0.0555318197 
0.0810115731  0.0906363831  0.1003000429  24.000000000  0.0001162026  0.0000975629  4.1852142678  864           0.0566638377 
0.0810115731  0.0906363831  0.1003000429  25.000000000  0.0000325806  0.0000183251  4.1905984349  900           0.0557147463 
0.0810115731  0.0906363831  0.1003000429  26.000000000  -0.000031846  -0.000068966  4.1994085974  936           0.0569309526 
0.0810115731  0.0906363831  0.1003000429  27.000000000  -0.000100016  0.0000426339  4.1710199912  972           0.0572383470 
0.0810115731  0.0906363831  0.1003000429  28.000000000  0.0002266947  0.0001319788  4.1607413027  1008          0.0556567576 
0.0810115731  0.0906363831  0.1003000429  29.000000000  0.0000072983  0.0000072925  4.2052821848  1044          0.0568528970 
Traceback (most recent call last):
  File "/usr/lib64/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/lib64/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/homes/55/tuan/KL/scripts/train.py", line 253, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
  File "/homes/55/tuan/KL/lib/misc.py", line 118, in accuracy
    p = network.predict(x)
  File "/homes/55/tuan/KL/algorithms.py", line 245, in predict
    preds += F.softmax(self.classifier(z),1)
  File "/homes/55/tuan/installations/tnew/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/homes/55/tuan/installations/tnew/lib64/python3.6/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/homes/55/tuan/installations/tnew/lib64/python3.6/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/homes/55/tuan/installations/tnew/lib64/python3.6/site-packages/torch/nn/modules/linear.py", line 94, in forward
    return F.linear(input, self.weight, self.bias)
  File "/homes/55/tuan/installations/tnew/lib64/python3.6/site-packages/torch/nn/functional.py", line 1753, in linear
    return torch._C._nn.linear(input, weight, bias)
KeyboardInterrupt
